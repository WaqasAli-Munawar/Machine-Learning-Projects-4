{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Face detection has become a very interesting problem in image processing and computer vision. Face mask detection has a range of applications from capturing the movement of the face to facial recognition which at first requires the face to be detected with very good precision. \n",
    "\n",
    "Face detection is more relevant today as it is not only used on images, but also in video applications like real-time surveillance and face detection in videos.\n",
    "\n",
    "High precision image classification is now possible with advances in convolutional networks. Pixel level information is often needed after face detection, which most face detection methods do not provide.\n",
    "\n",
    "Obtaining pixel-level detail has been a difficult part of semantic segmentation. Semantic segmentation is the process of assigning a label to each pixel in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process of Face Mask Detection with Machine Learning\n",
    "\n",
    "* `Step 1`: Extract face data for training.\n",
    "* `Step 2`: Train the classifier to classify faces in mask or labels without a mask.\n",
    "* `Step 3`: Detect faces while testing data using SSD face detector.\n",
    "* `Step 4`: Using the trained classifier, classify the detected faces.\n",
    "\n",
    "In the third step of the above process, we have to think about what is the SSD face detector? Well, the SSD is a **Single Shot Multibox Detector**. This is a technique used to detect objects in images using a single deep neural network.\n",
    "\n",
    "It is used for the detection of objects in an image. Using a basic architecture of the **VGG-16** architecture, the SSD can outperform other object detectors such as **YOLO** and **Faster R-CNN** in terms of speed and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import seaborn as sns\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"face-mask-detection-dataset/Medical mask/Medical mask/Medical Mask/annotations\"\n",
    "image_directory = \"face-mask-detection-dataset/Medical mask/Medical mask/Medical Mask/images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"face-mask-detection-dataset/train.csv\")\n",
    "df_test = pd.read_csv(\"face-mask-detection-dataset/submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Helper Functions\n",
    "\n",
    "We will start this task by creating two helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvNet = cv2.dnn.readNetFromCaffe('caffe-face-detector-opencv-pretrained-model/architecture.txt',\n",
    "                                 'caffe-face-detector-opencv-pretrained-model/weights.caffemodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getJSON(filePathandName):\n",
    "    with open(filePathandName,'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def adjust_gamma(image, gamma=1.0):\n",
    "    invGamma = 1.0 / gamma\n",
    "    table = np.array([((i / 255.0) ** invGamma) * 255 for i in np.arange(0, 256)])\n",
    "    return cv2.LUT(image.astype(np.uint8), table.astype(np.uint8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The `getJSON` function retrieves the json file containing the bounding box data in the training dataset.\n",
    "* The `adjust_gamma` function is a non-linear operation used to encode and decode luminance or tristimulus values in video or still image systems. Simply put, it is used to instil a little bit of light into the image. \n",
    "    * If `gamma <1`, the image will shift to the darker end of the spectrum and \n",
    "    * When `gamma> 1`, there will be more light in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing\n",
    "\n",
    "The next step is now to explore the JSON data provided for the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonfiles= []\n",
    "for i in os.listdir(directory):\n",
    "    jsonfiles.append(getJSON(os.path.join(directory,i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonfiles[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The Annotations field contains the data of all the faces present in a particular image.\n",
    "* There are different class names, but the real class names are `face_with_mask` and `face_no_mask`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `mask` and the `non_mask` labels, the bounding box data of the json files is extracted. The faces of a particular image are extracted and stored in the data list with its tag for the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "img_size = 124\n",
    "mask = ['face_with_mask']\n",
    "non_mask = [\"face_no_mask\"]\n",
    "labels={'mask':0,'without mask':1}\n",
    "for i in df[\"name\"].unique():\n",
    "    f = i+\".json\"\n",
    "    for j in getJSON(os.path.join(directory,f)).get(\"Annotations\"):\n",
    "        if j[\"classname\"] in mask:\n",
    "            x,y,w,h = j[\"BoundingBox\"]\n",
    "            img = cv2.imread(os.path.join(image_directory,i),1)\n",
    "            img = img[y:h,x:w]\n",
    "            img = cv2.resize(img,(img_size,img_size))\n",
    "            data.append([img,labels[\"mask\"]])\n",
    "        if j[\"classname\"] in non_mask:\n",
    "            x,y,w,h = j[\"BoundingBox\"]\n",
    "            img = cv2.imread(os.path.join(image_directory,i),1)\n",
    "            img = img[y:h,x:w]\n",
    "            img = cv2.resize(img,(img_size,img_size))    \n",
    "            data.append([img,labels[\"without mask\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = []\n",
    "for face in data:\n",
    "    if(face[1] == 0):\n",
    "        p.append(\"Mask\")\n",
    "    else:\n",
    "        p.append(\"No Mask\")\n",
    "sns.countplot(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualization above tells us that the **number of mask images> Number of images without a mask**, so this is an unbalanced dataset. But since we’re using a pre-trained SSD model, which is trained to detect unmasked faces, this imbalance wouldn’t matter much.\n",
    "\n",
    "But let’s reshape the data before training a neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "\n",
    "for features,label in data:\n",
    "    X.append(features)\n",
    "    Y.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)/255.0\n",
    "X = X.reshape(-1,124,124,3)\n",
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to train a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding = \"same\", activation='relu', input_shape=(124,124,3)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    " \n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam' ,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain,xval,ytrain,yval=train_test_split(X, Y,train_size=0.8,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  \n",
    "        samplewise_center=False,  \n",
    "        featurewise_std_normalization=False,  \n",
    "        samplewise_std_normalization=False,  \n",
    "        zca_whitening=False,    \n",
    "        rotation_range=15,    \n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,  \n",
    "        horizontal_flip=True,  \n",
    "        vertical_flip=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen.fit(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(datagen.flow(xtrain, ytrain, batch_size=32),\n",
    "                    steps_per_epoch=xtrain.shape[0]//32,\n",
    "                    epochs=50,\n",
    "                    verbose=1,\n",
    "                    validation_data=(xval, yval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing The Model\n",
    "\n",
    "The test dataset contains 1698 images and to evaluate the model we\n",
    "took a handful of images from this dataset as there are no face tags in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = ['1114.png','1504.jpg', '0072.jpg','0012.jpg','0353.jpg','1374.jpg']\n",
    "\n",
    "gamma = 2.0\n",
    "fig = plt.figure(figsize = (14,14))\n",
    "rows = 3\n",
    "cols = 2\n",
    "axes = []\n",
    "assign = {'0':'Mask','1':\"No Mask\"}\n",
    "\n",
    "for j,im in enumerate(test_images):\n",
    "    image =  cv2.imread(os.path.join(image_directory,im),1)\n",
    "    image =  adjust_gamma(image, gamma=gamma)\n",
    "    (h, w) = image.shape[:2]\n",
    "    blob = cv2.dnn.blobFromImage(cv2.resize(image, (300,300)), 1.0, (300, 300), (104.0, 177.0, 123.0))\n",
    "    cvNet.setInput(blob)\n",
    "    detections = cvNet.forward()\n",
    "    for i in range(0, detections.shape[2]):\n",
    "        try:\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "            frame = image[startY:endY, startX:endX]\n",
    "            confidence = detections[0, 0, i, 2]\n",
    "            if confidence > 0.2:\n",
    "                im = cv2.resize(frame,(img_size,img_size))\n",
    "                im = np.array(im)/255.0\n",
    "                im = im.reshape(1,124,124,3)\n",
    "                result = model.predict(im)\n",
    "                if result>0.5:\n",
    "                    label_Y = 1\n",
    "                else:\n",
    "                    label_Y = 0\n",
    "                cv2.rectangle(image, (startX, startY), (endX, endY), (0, 0, 255), 2)\n",
    "                cv2.putText(image,assign[str(label_Y)] , (startX, startY-10), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (36,255,12), 2)\n",
    "        \n",
    "        except:pass\n",
    "    axes.append(fig.add_subplot(rows, cols, j+1))\n",
    "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By analyzing the output above, we can observe that the whole system works well for faces that have spatial dominance. But fails in the case of images where the faces are small and take up less space in the overall image.\n",
    "\n",
    "For best results, different image preprocessing techniques can be used, or the confidence threshold can be kept lower, or one can try different blob sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
